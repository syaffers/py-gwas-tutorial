{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a222bc7f",
   "metadata": {},
   "source": [
    "# Explanation of the main script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358df0be",
   "metadata": {},
   "source": [
    "This is the main script for second tutorial from our comprehensive tutorial on GWAS and PRS.\n",
    "\n",
    "To run this script the following (b)files from the first tutorial are required: `HapMap_3_r3_12` (this bfile contain: `HapMap_3_r3_12.fam`, `HapMap_3_r3_12.bim`, and `HapMap_3_r3_12.bed`; you need all three), and `indepSNP.prune.in`.\n",
    "\n",
    "In this tutorial we are going to check for population stratification.\n",
    "\n",
    "We will do this as follows, the bfile (`HapMap_3_r3_12`) generated at the end of the previous tutorial (1_QC_GWAS) is going to checked for population stratification using data from the 1000 Genomes Project. Individuals with a non-European ethnic background will be removed.\n",
    "\n",
    "Furthermore, this tutorial will generate a covariate file which helps to adust for remaining population stratification within the European subjects.\n",
    "\n",
    "In order to complete this tutorial it is necessary to have generated the bfile `HapMap_3_r3_12` and the file `indepSNP.prune.in` from the previous tutorial.\n",
    "\n",
    "> **Author's note:** MDS means multidimensional scaling.\n",
    ">\n",
    "> <span style=\"color: #d35400\">**Author's note:**</span> This notebook folder will eventually take up 18GB on top of the 65GB pre-downloaded 1000 Genomes file. Make sure you have enough disk space to work with this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e012187e",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "> **Author's note:** below is some setup code I need to make the notebook shine a bit more. Some imports and styling for the plots later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1c11b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e260c0e5",
   "metadata": {},
   "source": [
    "Copy the (b)files from the previous tutorial to the current directory (see explanation of the main script).\n",
    "\n",
    "> **Author's note:** If you are looking for the files for this notebook and don't want to go through the first notebook, just open the first notebook in `1_QC_GWAS/` and click on the toolbar above: Cell > Run All, and come back to this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a543a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -p ../1_QC_GWAS/HapMap_3_r3_12.* .\n",
    "!cp -p ../1_QC_GWAS/indepSNP.prune.in ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850709f8",
   "metadata": {},
   "source": [
    "## Download 1000 Genomes data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc3409d",
   "metadata": {},
   "source": [
    "This file from the 1000 Genomes contains genetic data of 629 individuals from different ethnic backgrounds.\n",
    "\n",
    "Note, this file is quite large (>60 gigabyte).\n",
    "\n",
    "> **Author's note:** Downloading this file every single time you run this Docker container can be time-consuming (and probably bad for your SSD health) so it's wise not to keep re-running this cell. I have two options in mind to remedy this:\n",
    ">    1. Mount a volume from your host machine into the container and link (i.e. create a shortcut to) that file into this folder. I _recommend_ this option as you only need to download the file once in the host and you're less likely to accidentally remove the massive file when you terminate the container.\n",
    ">    2. Don't run this container with the `--rm` argument when you do `docker run`. This will keep the container in a dormant state and you can just restart the container and continue where you left off."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a7155c",
   "metadata": {},
   "source": [
    "**Option 1 (recommended):** Link the downloaded file from the mounted directory into this folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2d245f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ln -s /home/mambauser/workdir/data/ALL.2of4intersection.20100804.genotypes.vcf.gz /home/mambauser/workdir/2_Population_stratification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2fbc27",
   "metadata": {},
   "source": [
    "**Option 2:** Download the file per the original instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298d5eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget ftp://ftp-trace.ncbi.nih.gov/1000genomes/ftp/release/20100804/ALL.2of4intersection.20100804.genotypes.vcf.gz    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acc2c95",
   "metadata": {},
   "source": [
    "Convert vcf to Plink format.\n",
    "\n",
    "> **Author's note:** Grab a cup of tea... or 10. This is going to be a while. There are ~25.5M variants in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0caf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!plink --vcf ALL.2of4intersection.20100804.genotypes.vcf.gz --make-bed --out ALL.2of4intersection.20100804.genotypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0221619b",
   "metadata": {},
   "source": [
    "Noteworthy, the file `ALL.2of4intersection.20100804.genotypes.bim` contains SNPs without an rs-identifier, these SNPs are indicated with \".\". This can also be observed in the file `ALL.2of4intersection.20100804.genotypes.vcf.gz`. To check this file use this command:\n",
    "\n",
    "> **Author's note:** The original command, `zmore`, doesn't lend itself well to the Jupyter notebook environment. I instead crafted my own command line with pipes to fulfill the point that Andries was trying to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8be3649",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!zcat ALL.2of4intersection.20100804.genotypes.vcf.gz 2> /dev/null | head -n 100 | cut -f 1-5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39364360",
   "metadata": {},
   "source": [
    "The missing rs-identifiers in the 1000 Genomes data are not a problem for this tutorial.\n",
    "\n",
    "However, for good practice, we will assign unique indentifiers to the SNPs with a missing rs-identifier (i.e., the SNPs with \".\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0329bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!plink --bfile ALL.2of4intersection.20100804.genotypes --set-missing-var-ids '@:#[b37]$1,$2' --make-bed --out ALL.2of4intersection.20100804.genotypes_no_missing_IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e772950",
   "metadata": {},
   "source": [
    "## QC on 1000 Genomes data\n",
    "\n",
    "Remove variants based on missing genotype data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ea8c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "!plink --bfile ALL.2of4intersection.20100804.genotypes_no_missing_IDs --geno 0.2 --allow-no-sex --make-bed --out 1kG_MDS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c50b232",
   "metadata": {},
   "source": [
    "Remove individuals based on missing genotype data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d1dea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!plink --bfile 1kG_MDS --mind 0.2 --allow-no-sex --make-bed --out 1kG_MDS2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b24dba1",
   "metadata": {},
   "source": [
    "Remove variants based on missing genotype data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055d9cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!plink --bfile 1kG_MDS2 --geno 0.02 --allow-no-sex --make-bed --out 1kG_MDS3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae0910d",
   "metadata": {},
   "source": [
    "Remove individuals based on missing genotype data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34abb5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "!plink --bfile 1kG_MDS3 --mind 0.02 --allow-no-sex --make-bed --out 1kG_MDS4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beacc774",
   "metadata": {},
   "source": [
    "Remove variants based on MAF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9bc4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!plink --bfile 1kG_MDS4 --maf 0.05 --allow-no-sex --make-bed --out 1kG_MDS5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6676ca4e",
   "metadata": {},
   "source": [
    "## Equalize*\n",
    "\n",
    "Extract the variants present in HapMap dataset from the 1000 genomes dataset.\n",
    "\n",
    "> **Author's note (*):** The original script doesn't list this section as \"equalize\" but I felt that it belonged to its own subsection and not under QC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fb50f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!awk '{print$2}' HapMap_3_r3_12.bim > HapMap_SNPs.txt\n",
    "!plink --bfile 1kG_MDS5 --extract HapMap_SNPs.txt --make-bed --out 1kG_MDS6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7051c88",
   "metadata": {},
   "source": [
    "Extract the variants present in 1000 Genomes dataset from the HapMap dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f498799",
   "metadata": {},
   "outputs": [],
   "source": [
    "!awk '{print$2}' 1kG_MDS6.bim > 1kG_MDS6_SNPs.txt\n",
    "!plink --bfile HapMap_3_r3_12 --extract 1kG_MDS6_SNPs.txt --recode --make-bed --out HapMap_MDS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be3afd8",
   "metadata": {},
   "source": [
    "The datasets now contain the exact same variants.\n",
    "\n",
    "The datasets must have the same build. Change the build 1000 Genomes data build.\n",
    "\n",
    "> **Author's note:** Info about the columns of the `.map` file can be found here https://www.cog-genomics.org/plink/1.9/formats#map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950c1659",
   "metadata": {},
   "outputs": [],
   "source": [
    "!awk '{print$2,$4}' HapMap_MDS.map > buildhapmap.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f49df9",
   "metadata": {},
   "source": [
    "`buildhapmap.txt` contains one SNP-id and physical position per line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82006893",
   "metadata": {},
   "outputs": [],
   "source": [
    "!plink --bfile 1kG_MDS6 --update-map buildhapmap.txt --make-bed --out 1kG_MDS7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160d60b9",
   "metadata": {},
   "source": [
    "`1kG_MDS7` and `HapMap_MDS` now have the same build."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0761ab2",
   "metadata": {},
   "source": [
    "## Merge the HapMap and 1000 Genomes data sets\n",
    "\n",
    "Prior to merging 1000 Genomes data with the HapMap data we want to make sure that the files are mergeable, for this we conduct 3 steps:\n",
    "1. Make sure the reference genome is similar in the HapMap and the 1000 Genomes Project datasets.\n",
    "2. Resolve strand issues.\n",
    "3. Remove the SNPs which after the previous two steps still differ between datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c53ae3",
   "metadata": {},
   "source": [
    "The following steps are maybe quite technical in terms of commands, but we just compare the two data sets and make sure they correspond."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5790985c",
   "metadata": {},
   "source": [
    "### 1) Set reference genome "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9520a7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!awk '{print$2,$5}' 1kG_MDS7.bim > 1kg_ref-list.txt\n",
    "!plink --bfile HapMap_MDS --reference-allele 1kg_ref-list.txt --make-bed --out HapMap-adj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c33a9d",
   "metadata": {},
   "source": [
    "The `1kG_MDS7` and the `HapMap-adj` have the same reference genome for all SNPs.\n",
    "\n",
    "This command will generate some warnings for impossible A1 allele assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d106bf",
   "metadata": {},
   "source": [
    "### 2) Resolve strand issues.\n",
    "\n",
    "Check for potential strand issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e386e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "!awk '{print$2,$5,$6}' 1kG_MDS7.bim > 1kGMDS7_tmp\n",
    "!awk '{print$2,$5,$6}' HapMap-adj.bim > HapMap-adj_tmp\n",
    "!sort 1kGMDS7_tmp HapMap-adj_tmp |uniq -u > all_differences.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18415cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l all_differences.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2886f387",
   "metadata": {},
   "source": [
    "1624 differences between the files, some of these might be due to strand issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86062c09",
   "metadata": {},
   "source": [
    "#### Flip SNPs for resolving strand issues.\n",
    "\n",
    "Print SNP-identifier and remove duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf73dbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!awk '{print$1}' all_differences.txt | sort -u > flip_list.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133fa5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l flip_list.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad38dc87",
   "metadata": {},
   "source": [
    "Generates a file of 812 SNPs. These are the non-corresponding SNPs between the two files. \n",
    "\n",
    "Flip the 812 non-corresponding SNPs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dfc705",
   "metadata": {},
   "outputs": [],
   "source": [
    "!plink --bfile HapMap-adj --flip flip_list.txt --reference-allele 1kg_ref-list.txt --make-bed --out corrected_hapmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08af098",
   "metadata": {},
   "source": [
    "Check for SNPs which are still problematic after they have been flipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c247b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!awk '{print$2,$5,$6}' corrected_hapmap.bim > corrected_hapmap_tmp\n",
    "!sort 1kGMDS7_tmp corrected_hapmap_tmp |uniq -u  > uncorresponding_SNPs.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00b2548",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l uncorresponding_SNPs.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26efdde",
   "metadata": {},
   "source": [
    "This file demonstrates that there are 84 differences between the files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e39482",
   "metadata": {},
   "source": [
    "### 3) Remove problematic SNPs from HapMap and 1000 Genomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1138367",
   "metadata": {},
   "outputs": [],
   "source": [
    "!awk '{print$1}' uncorresponding_SNPs.txt | sort -u > SNPs_for_exlusion.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3a777d",
   "metadata": {},
   "source": [
    "The command above generates a list of the 42 SNPs which caused the 84 differences between the HapMap and the 1000 Genomes data sets after flipping and setting of the reference genome."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe23829",
   "metadata": {},
   "source": [
    "Remove the 42 problematic SNPs from both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d70f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!plink --bfile corrected_hapmap --exclude SNPs_for_exlusion.txt --make-bed --out HapMap_MDS2\n",
    "!plink --bfile 1kG_MDS7 --exclude SNPs_for_exlusion.txt --make-bed --out 1kG_MDS8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf492d61",
   "metadata": {},
   "source": [
    "Merge HapMap with 1000 Genomes Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f75cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!plink --bfile HapMap_MDS2 --bmerge 1kG_MDS8.bed 1kG_MDS8.bim 1kG_MDS8.fam --allow-no-sex --make-bed --out MDS_merge2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2668e3e4",
   "metadata": {},
   "source": [
    "Note, we are fully aware of the sample overlap between the HapMap and 1000 Genomes datasets. However, for the purpose of this tutorial this is not important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e3ad9c",
   "metadata": {},
   "source": [
    "## Perform MDS on HapMap-CEU data anchored by 1000 Genomes data.\n",
    "\n",
    "Using a set of pruned SNPs\n",
    "\n",
    "> **Author's note:** Info about the columns of the `.genome` file can be found here https://www.cog-genomics.org/plink/1.9/formats#genome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11db45d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!plink --bfile MDS_merge2 --extract indepSNP.prune.in --genome --out MDS_merge2\n",
    "!plink --bfile MDS_merge2 --read-genome MDS_merge2.genome --cluster --mds-plot 10 --out MDS_merge2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7b388e",
   "metadata": {},
   "source": [
    "### MDS-plot\n",
    "\n",
    "Download the file with population information of the 1000 genomes dataset.\n",
    "\n",
    "> **Author's note:** This file is small enough to be pre-included in the container image, so I thought I'd just download it and put it into the folder so you don't have to download it. I've left the original command here commented out for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720e9cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20100804/20100804.ALL.panel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfba5738",
   "metadata": {},
   "source": [
    "The file `20100804.ALL.panel` contains population codes of the individuals of 1000 genomes.\n",
    "\n",
    "> **Author's note:** There's only 629 samples in the 1000 Genomes dataset for this release ðŸ˜…. Don't worry about not finding 1000 samples anywhere in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ca175e",
   "metadata": {},
   "source": [
    "Convert population codes into superpopulation codes (i.e., AFR, AMR, ASN, and EUR)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d75dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!awk '{print$1,$1,$2}' 20100804.ALL.panel > race_1kG.txt\n",
    "!sed 's/JPT/ASN/g' race_1kG.txt > race_1kG2.txt\n",
    "!sed 's/ASW/AFR/g' race_1kG2.txt > race_1kG3.txt\n",
    "!sed 's/CEU/EUR/g' race_1kG3.txt > race_1kG4.txt\n",
    "!sed 's/CHB/ASN/g' race_1kG4.txt > race_1kG5.txt\n",
    "!sed 's/CHD/ASN/g' race_1kG5.txt > race_1kG6.txt\n",
    "!sed 's/YRI/AFR/g' race_1kG6.txt > race_1kG7.txt\n",
    "!sed 's/LWK/AFR/g' race_1kG7.txt > race_1kG8.txt\n",
    "!sed 's/TSI/EUR/g' race_1kG8.txt > race_1kG9.txt\n",
    "!sed 's/MXL/AMR/g' race_1kG9.txt > race_1kG10.txt\n",
    "!sed 's/GBR/EUR/g' race_1kG10.txt > race_1kG11.txt\n",
    "!sed 's/FIN/EUR/g' race_1kG11.txt > race_1kG12.txt\n",
    "!sed 's/CHS/ASN/g' race_1kG12.txt > race_1kG13.txt\n",
    "!sed 's/PUR/AMR/g' race_1kG13.txt > race_1kG14.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184624d6",
   "metadata": {},
   "source": [
    "Create a racefile of your own data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb169dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "!awk '{print$1,$2,\"OWN\"}' HapMap_MDS.fam > racefile_own.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7cf049",
   "metadata": {},
   "source": [
    "Concatenate racefiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99c978a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat race_1kG14.txt racefile_own.txt | sed -e '1i\\FID IID race' > racefile.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61e1b3d",
   "metadata": {},
   "source": [
    "Generate population stratification plot.\n",
    "\n",
    "> **Author's note:** Info about the columns of the `.mds` file can be found here https://www.cog-genomics.org/plink/1.9/formats#mds.\n",
    ">\n",
    "> The original dotted lines are centered at (-0.035, 0.035). I am not sure why but I'm guessing the earlier version of the MDS would place the data cluster around that locus. I have updated the dotted lines to segregate the EUR cluster instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bc1e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('MDS_merge2.mds', sep='\\s+')\n",
    "race = pd.read_csv('racefile.txt', sep=' ')\n",
    "\n",
    "datafile = pd.merge(data, race, how='left', left_on=('FID', 'IID'), right_on=('FID', 'IID'))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "\n",
    "datafile.loc[lambda d: d['race'] == 'EUR'].plot.scatter(\n",
    "    x='C1', y='C2', color='green', marker='o', ax=ax, label='EUR')\n",
    "datafile.loc[lambda d: d['race'] == 'ASN'].plot.scatter(\n",
    "    x='C1', y='C2', color='red', marker='o', ax=ax, label='ASN')\n",
    "datafile.loc[lambda d: d['race'] == 'AMR'].plot.scatter(\n",
    "    x='C1', y='C2', color='magenta', marker='o', ax=ax, label='AMR')\n",
    "datafile.loc[lambda d: d['race'] == 'AFR'].plot.scatter(\n",
    "    x='C1', y='C2', color='blue', marker='o', ax=ax, label='AFR')\n",
    "datafile.loc[lambda d: d['race'] == 'OWN'].plot.scatter(\n",
    "    x='C1', y='C2', color='k', marker='+', ax=ax, label='OWN')\n",
    "\n",
    "ax.set_xlim(-0.1, 0.2)\n",
    "ax.set_ylim(-0.15, 0.1)\n",
    "ax.set_xlabel('MDS Component 1')\n",
    "ax.set_ylabel('MDS Component 2')\n",
    "ax.legend()\n",
    "\n",
    "ax.axvline(-0.04, linestyle='dotted')\n",
    "ax.axhline(0.03, linestyle='dotted')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4146cb",
   "metadata": {},
   "source": [
    "The plot demonstrates that our 'own' data falls within the European group of the 1000 genomes data. Therefore, we do not have to remove subjects.\n",
    "\n",
    "For educational purposes however, we give scripts below to filter out population stratification outliers. Please execute the script below in order to generate the appropriate files for the next tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f149f03d",
   "metadata": {},
   "source": [
    "## Exclude ethnic outliers\n",
    "\n",
    "Select individuals in HapMap data below cut-off thresholds. The cut-off levels are not fixed thresholds but have to be determined based on the visualization of the first two dimensions. To exclude ethnic outliers, the thresholds need to be set around the cluster of population of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f478b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "!awk '{ if ($4 <-0.04 && $5 >0.03) print $1,$2 }' MDS_merge2.mds > EUR_MDS_merge2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a645b177",
   "metadata": {},
   "source": [
    "Extract these individuals in HapMap data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ad2c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "!plink --bfile HapMap_3_r3_12 --keep EUR_MDS_merge2 --make-bed --out HapMap_3_r3_13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba135c9b",
   "metadata": {},
   "source": [
    "Note, since our HapMap data did include any ethnic outliers, no individuls were removed at this step. However, if our data would have included individuals outside of the thresholds we set, then these individuals would have been removed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4583a6",
   "metadata": {},
   "source": [
    "## Create covariates based on MDS\n",
    "\n",
    "Perform an MDS ONLY on HapMap data without ethnic outliers. The values of the 10 MDS dimensions are subsequently used as covariates in the association analysis in the third tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b4ddc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!plink --bfile HapMap_3_r3_13 --extract indepSNP.prune.in --genome --out HapMap_3_r3_13\n",
    "!plink --bfile HapMap_3_r3_13 --read-genome HapMap_3_r3_13.genome --cluster --mds-plot 10 --out HapMap_3_r3_13_mds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc04348",
   "metadata": {},
   "source": [
    "Change the format of the `.mds` file into a plink covariate file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26da5845",
   "metadata": {},
   "outputs": [],
   "source": [
    "!awk '{print$1, $2, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13}' HapMap_3_r3_13_mds.mds > covar_mds.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ebf3bc",
   "metadata": {},
   "source": [
    "The values in `covar_mds.txt` will be used as covariates, to adjust for remaining population stratification, in the third tutorial where we will perform a genome-wide association analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d8c276",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e01eba",
   "metadata": {},
   "source": [
    "## Congratulations\n",
    "\n",
    "You have succesfully controlled your data for population stratification!\n",
    "\n",
    "For the next tutorial you need the following files:\n",
    "- `HapMap_3_r3_13` (the bfile, i.e., `HapMap_3_r3_13.bed`, `HapMap_3_r3_13.bim`, and `HapMap_3_r3_13.fam`\n",
    "- `covar_mds.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4860950",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
